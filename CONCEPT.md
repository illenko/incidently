# Incidently

AI-powered Slack bot for system health checks and incident analysis. Connects to your observability stack via MCP servers.

## What It Does

A Slack bot that connects to your observability tools (Grafana, Datadog, GCP Logging, Elasticsearch, etc.) via MCP servers, analyzes system health using an LLM (Gemini), and delivers structured summaries in a Slack thread. Operates in human-in-the-loop mode â€” the operator triggers checks and guides further analysis through conversation. All analysis logic lives in portable markdown playbooks â€” no code changes needed to adapt to a different stack.

## Stack

- **Go** â€” primary language
- **Google ADK Go** (`google.golang.org/adk`) â€” agent framework with built-in MCP toolset, ReAct loop, and LLM abstraction
- **slack-go/slack** â€” Slack bot (Socket Mode)
- **In-memory (sync.Map)** â€” session state (Redis later)

## Architecture

```
Slack (operator)
    â†“
Slack Gateway (socket mode, threads)
    â†“
Session Manager (per-thread conversation state)
    â†“
ADK Agent (Google ADK Go)
    â”œâ”€â”€ LLM (Gemini or other provider via ADK)
    â”œâ”€â”€ MCP Toolset (auto-discovers tools from configured MCP servers)
    â””â”€â”€ Playbooks (markdown instructions injected as agent instructions)
    â†“
Slack Gateway (reply in thread)
```

### Components

**Slack Gateway** â€” receives messages via Socket Mode. Distinguishes new commands from ongoing thread conversations. Always replies in thread.

**Session Manager** â€” each Slack thread = separate session. Stores message history for the ADK agent context. MVP uses sync.Map.

**ADK Agent** â€” core of the system. Google ADK Go handles the ReAct loop (LLM reasoning â†’ tool calls â†’ observation â†’ response), MCP tool discovery, and LLM communication. The agent's instructions are loaded from markdown playbooks.

**MCP Toolset** â€” ADK's built-in `McpToolset` connects to external MCP servers, discovers available tools, and makes them callable by the agent. No custom MCP client code needed.

**Playbooks** â€” markdown files loaded as agent instructions (details below).

## Package Structure

```
/cmd/bot              â€” entrypoint
/internal/
  /slack              â€” gateway, message parsing, thread management
  /session            â€” session manager, conversation state
  /agent              â€” ADK agent setup, playbook loader, MCP toolset config
  /config             â€” config loader

/playbooks/           â€” markdown instructions for the agent (portable across installations)
  health-check.md     â€” example: full system health check
  drill-down.md       â€” example: deep dive into a specific component

/config/
  config.yaml         â€” Slack, LLM, and MCP server connection settings
```

## Playbooks

Playbooks are **markdown files** â€” natural language instructions loaded as the ADK agent's system instructions. They describe what to check, in what order, and how to interpret results. They reference concrete dashboard UIDs, panel names, and log queries so the agent knows exactly which MCP tool calls to make.

The Go application sets up an ADK agent with: playbook text as instructions, MCP toolsets pointing to configured servers, and the LLM provider. ADK handles the ReAct loop â€” the agent reads its instructions, decides which tools to call, observes results, and formulates a response. All domain logic lives in the playbooks.

This makes the project **portable**: to adapt Incidently for a different system, write new playbooks and update MCP server config â€” no Go code changes needed.

### How It Works

1. Operator triggers `/health`
2. Bot loads `playbooks/health-check.md` as agent instructions
3. ADK agent is initialized with the playbook instructions + MCP toolsets (connected to servers from config)
4. Operator's message is passed to the agent
5. ADK runs the ReAct loop: agent reads playbook â†’ calls MCP tools (dashboards, logs) â†’ observes results â†’ reasons about next step â†’ repeats until done
6. Agent's final response is posted to Slack thread
7. On follow-up messages in the thread, the agent has full conversation history and can load drill-down playbooks as needed

### Playbook Routing

The agent needs to know which playbook to use. MVP approach: **single root playbook + slash-command shortcuts**.

One main playbook is always loaded as the agent's instructions. It contains all analysis scenarios and describes when to use each one. The agent's ReAct loop decides which section to execute based on the operator's message.

Slash commands act as shortcuts that load specific playbooks with a pre-filled intent:

- `/health` â†’ loads `health-check.md` + sends "perform a health check"
- `/playbook infra` â†’ loads a specific playbook by name
- Free-text message in thread â†’ agent already has the playbook + session context, figures out what to do

The root playbook contains routing logic as part of its instructions:

```markdown
## When the operator asks for a health check
Perform the full overview (Steps 1-3 below).

## When the operator asks to dig into a specific component
Skip the overview. Go directly to detailed analysis
of that component (Step 4).

## When the operator asks a follow-up question
Use the data already collected in this session to answer.
If you need more data, query it.
```

This keeps things simple â€” one playbook, one agent, no router layer. When the number of playbooks grows and they become too large for a single context, introduce a lightweight router agent that picks the right playbook based on the operator's message and available playbook descriptions.

### Playbook Format

Playbooks are written as agent instructions â€” the same way you'd explain the analysis process to a new engineer, but with exact technical identifiers needed for automation.

Example (`playbooks/health-check.md`):

```markdown
# Health Check

You are performing a system health check. Use the available MCP tools
to query dashboards and search logs.

## Step 1: Overview

Query dashboard UID `main-overview`. Pull these panels for the last
15 minutes:

- **Error Rate** â€” separate system errors (5xx, timeouts) from client
  errors (4xx). If only client errors spiked, the system is healthy â€”
  note the source but don't flag it.
- **Latency p99**
- **Throughput**

Compare each metric to the same time last week. Flag as warning if
deviation > 20%, critical if > 50%. If throughput is lower but it's
a weekend or night â€” that's likely normal, note it but don't flag.

## Step 2: Dependencies

Query dashboard UID `dependencies-overview`. This shows all external
dependencies with their current success rate and response time.

Identify dependencies where success rate deviates more than 20% from
their average over the past week.

For each degraded dependency:
1. Drill into its dedicated dashboard (linked from the overview)
2. Check which error codes are increasing
3. Search logs for that dependency name â€” look for maintenance
   notices or upstream errors
4. Identify which consumers are affected

## Step 3: Infrastructure

Query dashboard UID `infra-001`:
- **Pod Restarts** panel â€” last 1 hour
- **Queue Lag** panel â€” last 15 minutes
- **DB Connections** panel â€” last 15 minutes

If pods restarted, check if there was a deployment in the last hour.

## Output

Present results grouped by: Application, Dependencies, Infrastructure.
Use âœ… âš ï¸ ğŸ”´ indicators. Only show details for anomalies.
Always suggest what to check next if there are warnings or critical items.
Include dashboard links where relevant.
```

### Drill-Down Playbooks

When the operator asks to dig deeper, the agent loads the appropriate drill-down playbook. These are domain-specific â€” each installation writes its own based on its architecture. Examples:

- Detailed dependency analysis: error breakdown, timeline, affected consumers, log search
- Infrastructure deep dive: pod restarts with reasons, queue consumer details, DB slow queries, deployment correlation
- Service-specific analysis: per-service metrics, error patterns, upstream/downstream impact

Each drill-down playbook contains its own dashboard UIDs, log queries, and analysis logic.

### Key Principles

**Anomaly â‰  Incident.** Playbooks should instruct the agent to distinguish between real system problems and expected noise (e.g. client-side errors, normal traffic variance, scheduled maintenance).

**Dynamic baseline.** No hardcoded thresholds. Compare to same weekday last week. Minimum sample size to avoid false positives on low traffic.

**Concrete identifiers.** Every playbook contains exact dashboard UIDs, panel names, log queries, and filter values. The agent must not guess â€” it reads the playbook and knows exactly what MCP calls to make.

## Config

Only minimal static configuration that the engine needs to operate:

```yaml
# config/config.yaml

slack:
  app_token: "${SLACK_APP_TOKEN}"
  bot_token: "${SLACK_BOT_TOKEN}"

llm:
  provider: "gemini"
  model: "gemini-2.5-pro"
  api_key: "${GEMINI_API_KEY}"

mcp_servers:
  - name: grafana
    url: "https://grafana-mcp.internal.example.com/sse"

  - name: gcp-logging
    url: "https://gcp-logging-mcp.internal.example.com/sse"
```

MCP servers are deployed and managed separately. The bot connects to them via SSE as a client. ADK's `McpToolset` handles connection, tool discovery, and execution.

## Slack UX

### Trigger â€” Manual

```
Operator: /health

Bot: ğŸ” Checking system health...

Bot: ğŸ¥ System Health Check â€” 14:31 UTC

     Application
     âœ… Error rate: 0.8%
     âœ… Latency p99: 1.1s
     âœ… Throughput: 312 rps (last week: 305 rps)

     Dependencies
     âœ… ServiceX: 99.1%
     âš ï¸ ServiceZ: 94.2% â€” elevated timeouts
     âœ… Others: normal

     Infrastructure
     âœ… CPU/Memory: normal
     âœ… Queue lag: 12ms
     âœ… DB connections: 45/200

     âš ï¸ 1 warning: ServiceZ

     Dig into ServiceZ?
```

### Conversation â€” Natural Language in Thread

```
Operator: let's see

Bot: ServiceZ last hour:
     ğŸ“‰ Success rate: 94.2% â†’ trend stable
     âŒ Top error: read timeout 10s â€” 84 times
     ğŸ”€ Affected: ConsumerA, ConsumerB
     ğŸ“Š Volume: 4.2% of total traffic
     Not critical, worth monitoring.
     ğŸ“ [ServiceZ dashboard link]
```

### Launch Variations

- `/health` â€” full health check
- `check dependencies` â€” specific area
- `what's up with ServiceX?` â€” specific component
- `how are we compared to yesterday?` â€” comparison

### UX Principles

- Bot always replies **in thread**
- **Natural language** â€” "check providers" works same as slash commands
- **Dashboard links** â€” bot complements your observability tools, doesn't replace them
- **Suggests next steps** after every response
- **Timers** â€” "remind me to check in 20 minutes"

## MVP Scope

### Included

- Manual trigger via Slack (`/health`, natural language)
- Health Check playbook with drill-down support
- Threaded conversation with session context
- Dynamic baseline (weekly comparison)
- Read-only â€” analysis only, no automated actions

### Not in MVP

- Automatic alert triggers (add later â€” one webhook)
- Automated actions (restarts, scaling, failover)
- Session persistence (in-memory, lost on restart)
- Multi-user / access control
- Post-mortem generation

## Post-MVP Roadmap

1. **Alert triggers** â€” alert webhook â†’ bot auto-starts analysis
2. **Post-mortem draft** â€” bot generates summary from thread after incident closure
3. **Correlations** â€” temporal, deploy, cross-service
4. **Incident history** â€” search similar past incidents
5. **Redis** â€” persistent sessions
6. **Multi-user** â€” full on-call team interaction